{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Keypoint Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = pd.read_pickle(\"./dataset/labels.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write txt files for torch training. Downloaded data is in ./dataset/cropped_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import mkdir\n",
    "import shutil\n",
    "#split from paper\n",
    "d1_val = ['d1_02_06_2020', 'd1_02_16_2020', 'd1_02_22_2020']\n",
    "d1_test = ['d1_03_03_2020', 'd1_03_19_2020', 'd1_03_23_2020', 'd1_03_27_2020', 'd1_03_28_2020', 'd1_03_30_2020', 'd1_03_31_2020']\n",
    "\n",
    "d2_val = ['d2_02_03_2021', 'd2_02_05_2021']\n",
    "d2_test = ['d2_03_03_2020', 'd2_02_10_2021', 'd2_02_03_2021_2']\n",
    "#new split:\n",
    "val = d1_val + d2_val\n",
    "test = d1_test + d2_test\n",
    "mkdir(\"./dataset/train\")\n",
    "mkdir(\"./dataset/val\")\n",
    "mkdir(\"./dataset/test\")\n",
    "for index, row in labels.iterrows():\n",
    "    typ = \"train\"\n",
    "    if row[\"img_folder\"] in val:\n",
    "        typ = \"val\"\n",
    "    elif row[\"img_folder\"] in test:\n",
    "        typ = \"test\"\n",
    "    img_path = \"./dataset/cropped_images/800/{}/{}\".format(row[\"img_folder\"],row[\"img_name\"])\n",
    "    dest = \"./dataset/{}/{}\".format(typ,row[\"img_name\"])\n",
    "    shutil.copyfile(img_path,dest)\n",
    "    txt = open(\"./dataset/{}/{}.txt\".format(typ,row[\"img_name\"].split(\".\")[0]),\"w\")\n",
    "    #skip first 3 labels and only one class\n",
    "    for xy in row[\"xy\"][3:]:\n",
    "        txt.write(\"0 {} {} 1 1\\n\".format(xy[0],xy[1]))\n",
    "    txt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from IPython.display import Image, clear_output\n",
    "from utils.plots import plot_results\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Hub\n",
    "import torch\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model.eval()\n",
    "# Images\n",
    "dir = 'dataset/cropped_images/800'\n",
    "imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')]  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "results.show()  # or .show(), .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('yolov5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f61a95a29cde3225c6111fb7f889344bac656657078bba910ddd24d849010466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
